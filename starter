{"metadata":{"kernelspec":{"language":"python","display_name":"Python 3","name":"python3"},"language_info":{"name":"python","version":"3.10.14","mimetype":"text/x-python","codemirror_mode":{"name":"ipython","version":3},"pygments_lexer":"ipython3","nbconvert_exporter":"python","file_extension":".py"},"kaggle":{"accelerator":"gpu","dataSources":[{"sourceId":88742,"databundleVersionId":10173359,"sourceType":"competition"}],"dockerImageVersionId":30804,"isInternetEnabled":true,"language":"python","sourceType":"notebook","isGpuEnabled":true}},"nbformat_minor":4,"nbformat":4,"cells":[{"cell_type":"code","source":"# This Python 3 environment comes with many helpful analytics libraries installed\n# It is defined by the kaggle/python Docker image: https://github.com/kaggle/docker-python\n# For example, here's several helpful packages to load\n\nimport numpy as np # linear algebra\nimport pandas as pd # data processing, CSV file I/O (e.g. pd.read_csv)\n\n# Input data files are available in the read-only \"../input/\" directory\n# For example, running this (by clicking run or pressing Shift+Enter) will list all files under the input directory\n\nimport os\nfor dirname, _, filenames in os.walk('/kaggle/input'):\n    for filename in filenames:\n        print(os.path.join(dirname, filename))\n\n# You can write up to 20GB to the current directory (/kaggle/working/) that gets preserved as output when you create a version using \"Save & Run All\" \n# You can also write temporary files to /kaggle/temp/, but they won't be saved outside of the current session","metadata":{"_uuid":"8f2839f25d086af736a60e9eeb907d3b93b6e0e5","_cell_guid":"b1076dfc-b9ad-4769-8c92-a6c4dae69d19","trusted":true,"execution":{"iopub.status.busy":"2025-01-06T13:16:21.838023Z","iopub.execute_input":"2025-01-06T13:16:21.838425Z","iopub.status.idle":"2025-01-06T13:16:21.846389Z","shell.execute_reply.started":"2025-01-06T13:16:21.838394Z","shell.execute_reply":"2025-01-06T13:16:21.845577Z"}},"outputs":[{"name":"stdout","text":"/kaggle/input/rohlik-sales-forecasting-challenge-v2/calendar.csv\n/kaggle/input/rohlik-sales-forecasting-challenge-v2/test_weights.csv\n/kaggle/input/rohlik-sales-forecasting-challenge-v2/inventory.csv\n/kaggle/input/rohlik-sales-forecasting-challenge-v2/sales_train.csv\n/kaggle/input/rohlik-sales-forecasting-challenge-v2/sales_test.csv\n/kaggle/input/rohlik-sales-forecasting-challenge-v2/solution.csv\n","output_type":"stream"}],"execution_count":23},{"cell_type":"code","source":"import matplotlib.pyplot as plt\nimport seaborn as sns\nimport torch\n#from iTransformer import iTransformer","metadata":{"trusted":true,"execution":{"iopub.status.busy":"2025-01-06T13:16:21.848193Z","iopub.execute_input":"2025-01-06T13:16:21.848411Z","iopub.status.idle":"2025-01-06T13:16:21.858269Z","shell.execute_reply.started":"2025-01-06T13:16:21.848389Z","shell.execute_reply":"2025-01-06T13:16:21.857595Z"}},"outputs":[],"execution_count":24},{"cell_type":"code","source":"# Load data\ntrain = pd.read_csv('/kaggle/input/rohlik-sales-forecasting-challenge-v2/sales_train.csv')\ncalendar = pd.read_csv('/kaggle/input/rohlik-sales-forecasting-challenge-v2/calendar.csv')\ninventory = pd.read_csv('/kaggle/input/rohlik-sales-forecasting-challenge-v2/inventory.csv')","metadata":{"trusted":true,"execution":{"iopub.status.busy":"2025-01-06T13:16:21.859154Z","iopub.execute_input":"2025-01-06T13:16:21.859373Z","iopub.status.idle":"2025-01-06T13:16:25.889484Z","shell.execute_reply.started":"2025-01-06T13:16:21.859350Z","shell.execute_reply":"2025-01-06T13:16:25.888579Z"}},"outputs":[],"execution_count":25},{"cell_type":"code","source":"train = train.iloc[:10000]","metadata":{"trusted":true,"execution":{"iopub.status.busy":"2025-01-06T13:16:25.890636Z","iopub.execute_input":"2025-01-06T13:16:25.891018Z","iopub.status.idle":"2025-01-06T13:16:25.895436Z","shell.execute_reply.started":"2025-01-06T13:16:25.890956Z","shell.execute_reply":"2025-01-06T13:16:25.894439Z"}},"outputs":[],"execution_count":26},{"cell_type":"code","source":"#train = train.merge(inventory, on = ['unique_id','warehouse'], how = 'left')\n#train = train.merge(calendar, on = ['date','warehouse'], how = 'left')","metadata":{"trusted":true,"execution":{"iopub.status.busy":"2025-01-06T13:16:25.898362Z","iopub.execute_input":"2025-01-06T13:16:25.899118Z","iopub.status.idle":"2025-01-06T13:16:25.905028Z","shell.execute_reply.started":"2025-01-06T13:16:25.899077Z","shell.execute_reply":"2025-01-06T13:16:25.904191Z"}},"outputs":[],"execution_count":27},{"cell_type":"code","source":"train['total_orders'] = train['total_orders'].interpolate(method='polynomial', order=2)\ntrain['sales'] = train['sales'].interpolate(method='polynomial', order=2)","metadata":{"trusted":true,"execution":{"iopub.status.busy":"2025-01-06T13:16:25.906015Z","iopub.execute_input":"2025-01-06T13:16:25.906321Z","iopub.status.idle":"2025-01-06T13:16:25.918662Z","shell.execute_reply.started":"2025-01-06T13:16:25.906294Z","shell.execute_reply":"2025-01-06T13:16:25.917873Z"}},"outputs":[],"execution_count":28},{"cell_type":"code","source":"# Convert date to datetime\ntrain['date'] = pd.to_datetime(train['date'])\n\n# Temporal features\ntrain['day_of_week'] = train['date'].dt.dayofweek\ntrain['month'] = train['date'].dt.month\ntrain['year'] = train['date'].dt.year\ntrain['is_weekend'] = train['day_of_week'].isin([5, 6]).astype(int)\n\n# Discount features\ndiscount_columns = [col for col in train.columns if 'type_' in col]\ntrain['max_discount'] = train[discount_columns].max(axis=1)\ntrain['total_discounts'] = train[discount_columns].sum(axis=1)\ntrain['is_discounted'] = (train['max_discount'] > 0).astype(int)\n\n# Lag features\ntrain = train.sort_values(by=['unique_id', 'date'])\ntrain['sales_lag_1'] = train.groupby('unique_id')['sales'].shift(1)\ntrain['sales_lag_7'] = train.groupby('unique_id')['sales'].shift(7)\n\n# Rolling statistics\ntrain['rolling_mean_7'] = train.groupby('unique_id')['sales'].transform(lambda x: x.rolling(window=7, min_periods=1).mean())\ntrain['rolling_std_7'] = train.groupby('unique_id')['sales'].transform(lambda x: x.rolling(window=7, min_periods=1).std())\n\n# Product and warehouse features\ntrain['warehouse_sales_mean'] = train.groupby('warehouse')['sales'].transform('mean')\ntrain['warehouse_sales_std'] = train.groupby('warehouse')['sales'].transform('std')\n\n# Drop rows with NaN values created by lag features\ntrain = train.dropna(subset=['sales_lag_1', 'sales_lag_7'])","metadata":{"trusted":true,"execution":{"iopub.status.busy":"2025-01-06T13:16:25.919572Z","iopub.execute_input":"2025-01-06T13:16:25.919877Z","iopub.status.idle":"2025-01-06T13:16:25.980997Z","shell.execute_reply.started":"2025-01-06T13:16:25.919833Z","shell.execute_reply":"2025-01-06T13:16:25.980326Z"}},"outputs":[],"execution_count":29},{"cell_type":"code","source":"# remove columns by name\ntrain = train.drop('warehouse', axis=1)","metadata":{"trusted":true,"execution":{"iopub.status.busy":"2025-01-06T13:16:25.981968Z","iopub.execute_input":"2025-01-06T13:16:25.982234Z","iopub.status.idle":"2025-01-06T13:16:25.988153Z","shell.execute_reply.started":"2025-01-06T13:16:25.982211Z","shell.execute_reply":"2025-01-06T13:16:25.987197Z"}},"outputs":[],"execution_count":30},{"cell_type":"code","source":"import lightgbm as lgb\nfrom sklearn.model_selection import train_test_split\n\n# Define features and target\nfeatures = [col for col in train.columns if col not in ['sales', 'date', 'unique_id']]\ntarget = 'sales'\n\n# Split data\nX_train, X_val, y_train, y_val = train_test_split(train[features], train[target], test_size=0.2, random_state=42)\n\n# Train LightGBM model\nmodel = lgb.LGBMRegressor()\nmodel.fit(X_train, y_train)\n\n# Get feature importance\nfeature_importance = pd.DataFrame({\n    'feature': features,\n    'importance': model.feature_importances_\n}).sort_values(by='importance', ascending=False)\n\nprint(feature_importance)","metadata":{"trusted":true,"execution":{"iopub.status.busy":"2025-01-06T13:16:25.989170Z","iopub.execute_input":"2025-01-06T13:16:25.989415Z","iopub.status.idle":"2025-01-06T13:16:26.128973Z","shell.execute_reply.started":"2025-01-06T13:16:25.989391Z","shell.execute_reply":"2025-01-06T13:16:26.128062Z"}},"outputs":[{"name":"stdout","text":"[LightGBM] [Info] Auto-choosing row-wise multi-threading, the overhead of testing was 0.001923 seconds.\nYou can set `force_row_wise=true` to remove the overhead.\nAnd if memory is not enough, you can set `force_col_wise=true`.\n[LightGBM] [Info] Total Bins 2074\n[LightGBM] [Info] Number of data points in the train set: 7949, number of used features: 19\n[LightGBM] [Info] Start training from score 62.459591\n                 feature  importance\n20         rolling_std_7         591\n19        rolling_mean_7         521\n17           sales_lag_1         426\n0           total_orders         332\n18           sales_lag_7         284\n10           day_of_week         221\n1        sell_price_main         126\n11                 month         123\n14          max_discount         111\n12                  year          81\n3        type_0_discount          76\n2           availability          71\n21  warehouse_sales_mean          19\n9        type_6_discount          16\n22   warehouse_sales_std           2\n6        type_3_discount           0\n5        type_2_discount           0\n4        type_1_discount           0\n8        type_5_discount           0\n13            is_weekend           0\n7        type_4_discount           0\n15       total_discounts           0\n16         is_discounted           0\n","output_type":"stream"}],"execution_count":31},{"cell_type":"code","source":"from sklearn.metrics import mean_absolute_error\n# Select top N features\ntop_features = feature_importance.head(15)['feature'].tolist()\n\n# Retrain model with top features\nmodel = lgb.LGBMRegressor()\nmodel.fit(X_train[top_features], y_train)\n\n# Evaluate on validation set\ny_pred = model.predict(X_val[top_features])\nmae = mean_absolute_error(y_val, y_pred)\nprint(f'Mean Absolute Error with top features: {mae}')","metadata":{"trusted":true,"execution":{"iopub.status.busy":"2025-01-06T13:16:55.685778Z","iopub.execute_input":"2025-01-06T13:16:55.686118Z","iopub.status.idle":"2025-01-06T13:16:55.797929Z","shell.execute_reply.started":"2025-01-06T13:16:55.686089Z","shell.execute_reply":"2025-01-06T13:16:55.796953Z"}},"outputs":[{"name":"stdout","text":"[LightGBM] [Info] Auto-choosing row-wise multi-threading, the overhead of testing was 0.000416 seconds.\nYou can set `force_row_wise=true` to remove the overhead.\nAnd if memory is not enough, you can set `force_col_wise=true`.\n[LightGBM] [Info] Total Bins 1836\n[LightGBM] [Info] Number of data points in the train set: 7949, number of used features: 15\n[LightGBM] [Info] Start training from score 62.459591\nMean Absolute Error with top features: 13.67816695110636\n","output_type":"stream"}],"execution_count":33},{"cell_type":"code","source":"","metadata":{"trusted":true},"outputs":[],"execution_count":null}]}
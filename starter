{"metadata":{"kernelspec":{"language":"python","display_name":"Python 3","name":"python3"},"language_info":{"name":"python","version":"3.10.14","mimetype":"text/x-python","codemirror_mode":{"name":"ipython","version":3},"pygments_lexer":"ipython3","nbconvert_exporter":"python","file_extension":".py"},"kaggle":{"accelerator":"none","dataSources":[{"sourceId":88742,"databundleVersionId":10173359,"sourceType":"competition"}],"dockerImageVersionId":30804,"isInternetEnabled":true,"language":"python","sourceType":"notebook","isGpuEnabled":false}},"nbformat_minor":4,"nbformat":4,"cells":[{"cell_type":"code","source":"# This Python 3 environment comes with many helpful analytics libraries installed\n# It is defined by the kaggle/python Docker image: https://github.com/kaggle/docker-python\n# For example, here's several helpful packages to load\n\nimport numpy as np # linear algebra\nimport pandas as pd # data processing, CSV file I/O (e.g. pd.read_csv)\n\n# Input data files are available in the read-only \"../input/\" directory\n# For example, running this (by clicking run or pressing Shift+Enter) will list all files under the input directory\n\nimport os\nfor dirname, _, filenames in os.walk('/kaggle/input'):\n    for filename in filenames:\n        print(os.path.join(dirname, filename))\n\n# You can write up to 20GB to the current directory (/kaggle/working/) that gets preserved as output when you create a version using \"Save & Run All\" \n# You can also write temporary files to /kaggle/temp/, but they won't be saved outside of the current session","metadata":{"_uuid":"92a31eb7-5169-46af-93be-7d7c5e79d6e3","_cell_guid":"235481d4-8665-4bb0-a69d-ec6c0541a289","trusted":true,"collapsed":false,"execution":{"iopub.status.busy":"2025-02-05T15:58:53.477802Z","iopub.execute_input":"2025-02-05T15:58:53.478401Z","iopub.status.idle":"2025-02-05T15:58:53.491174Z","shell.execute_reply.started":"2025-02-05T15:58:53.478347Z","shell.execute_reply":"2025-02-05T15:58:53.489374Z"},"jupyter":{"outputs_hidden":false}},"outputs":[{"name":"stdout","text":"/kaggle/input/rohlik-sales-forecasting-challenge-v2/calendar.csv\n/kaggle/input/rohlik-sales-forecasting-challenge-v2/test_weights.csv\n/kaggle/input/rohlik-sales-forecasting-challenge-v2/inventory.csv\n/kaggle/input/rohlik-sales-forecasting-challenge-v2/sales_train.csv\n/kaggle/input/rohlik-sales-forecasting-challenge-v2/sales_test.csv\n/kaggle/input/rohlik-sales-forecasting-challenge-v2/solution.csv\n","output_type":"stream"}],"execution_count":61},{"cell_type":"code","source":"import lightgbm as lgb\nfrom sklearn.metrics import mean_absolute_error\nfrom sklearn.model_selection import TimeSeriesSplit\nimport matplotlib.pyplot as plt\nimport gc","metadata":{"_uuid":"837177dd-7de1-4c0a-afc2-51358468d365","_cell_guid":"c4bfa52b-530c-4d71-a42d-20b3e6297b40","trusted":true,"collapsed":false,"execution":{"iopub.status.busy":"2025-02-05T15:58:53.494355Z","iopub.execute_input":"2025-02-05T15:58:53.494893Z","iopub.status.idle":"2025-02-05T15:58:53.513646Z","shell.execute_reply.started":"2025-02-05T15:58:53.494840Z","shell.execute_reply":"2025-02-05T15:58:53.511832Z"},"jupyter":{"outputs_hidden":false}},"outputs":[],"execution_count":62},{"cell_type":"code","source":"# Adjust file paths as necessary\nsales_train = pd.read_csv('/kaggle/input/rohlik-sales-forecasting-challenge-v2/sales_train.csv', parse_dates=['date'])\nsales_test = pd.read_csv('/kaggle/input/rohlik-sales-forecasting-challenge-v2/sales_test.csv', parse_dates=['date'])\ninventory = pd.read_csv('/kaggle/input/rohlik-sales-forecasting-challenge-v2/inventory.csv')\ncalendar = pd.read_csv('/kaggle/input/rohlik-sales-forecasting-challenge-v2/calendar.csv', parse_dates=['date'])\ntest_weights = pd.read_csv('/kaggle/input/rohlik-sales-forecasting-challenge-v2/test_weights.csv')","metadata":{"_uuid":"f7a0a25e-7020-4263-a5f0-6f6268f1418e","_cell_guid":"cf50a8ee-1257-42e0-b197-25818133100f","trusted":true,"collapsed":false,"execution":{"iopub.status.busy":"2025-02-05T15:58:53.515772Z","iopub.execute_input":"2025-02-05T15:58:53.516704Z","iopub.status.idle":"2025-02-05T15:58:59.806358Z","shell.execute_reply.started":"2025-02-05T15:58:53.516562Z","shell.execute_reply":"2025-02-05T15:58:59.804756Z"},"jupyter":{"outputs_hidden":false}},"outputs":[],"execution_count":63},{"cell_type":"code","source":"# Merge inventory data to sales (on unique_id and warehouse)\ntrain = sales_train.merge(inventory, on=['unique_id', 'warehouse'], how='left')\ntest = sales_test.merge(inventory, on=['unique_id', 'warehouse'], how='left')\n\n# Merge calendar information to train and test data (on date and warehouse)\ntrain = train.merge(calendar, on=['date', 'warehouse'], how='left')\ntest = test.merge(calendar, on=['date', 'warehouse'], how='left')","metadata":{"_uuid":"db60eafa-70db-4391-957f-873c47bf2fa8","_cell_guid":"cd3d0482-fbb1-4ec7-8ff4-f55a1ed3464f","trusted":true,"collapsed":false,"execution":{"iopub.status.busy":"2025-02-05T15:58:59.809732Z","iopub.execute_input":"2025-02-05T15:58:59.810238Z","iopub.status.idle":"2025-02-05T15:59:03.296519Z","shell.execute_reply.started":"2025-02-05T15:58:59.810168Z","shell.execute_reply":"2025-02-05T15:59:03.294837Z"},"jupyter":{"outputs_hidden":false}},"outputs":[],"execution_count":64},{"cell_type":"code","source":"train = train.iloc[:20000]","metadata":{"_uuid":"dd3927f4-ec48-40ed-a259-e194fca3d2fc","_cell_guid":"432a2def-c736-4b78-87df-69d64e0d2ad1","trusted":true,"collapsed":false,"execution":{"iopub.status.busy":"2025-02-05T15:59:03.298046Z","iopub.execute_input":"2025-02-05T15:59:03.298854Z","iopub.status.idle":"2025-02-05T15:59:03.305664Z","shell.execute_reply.started":"2025-02-05T15:59:03.298789Z","shell.execute_reply":"2025-02-05T15:59:03.304175Z"},"jupyter":{"outputs_hidden":false}},"outputs":[],"execution_count":65},{"cell_type":"code","source":"train.columns","metadata":{"_uuid":"7047f907-a14d-414e-b1b8-81af439fb6b8","_cell_guid":"9a390a64-ab30-4d18-ae32-750b48ddeedd","trusted":true,"collapsed":false,"execution":{"iopub.status.busy":"2025-02-05T15:59:03.307743Z","iopub.execute_input":"2025-02-05T15:59:03.308109Z","iopub.status.idle":"2025-02-05T15:59:03.334791Z","shell.execute_reply.started":"2025-02-05T15:59:03.308074Z","shell.execute_reply":"2025-02-05T15:59:03.333104Z"},"jupyter":{"outputs_hidden":false}},"outputs":[{"execution_count":66,"output_type":"execute_result","data":{"text/plain":"Index(['unique_id', 'date', 'warehouse', 'total_orders', 'sales',\n       'sell_price_main', 'availability', 'type_0_discount', 'type_1_discount',\n       'type_2_discount', 'type_3_discount', 'type_4_discount',\n       'type_5_discount', 'type_6_discount', 'product_unique_id', 'name',\n       'L1_category_name_en', 'L2_category_name_en', 'L3_category_name_en',\n       'L4_category_name_en', 'holiday_name', 'holiday', 'shops_closed',\n       'winter_school_holidays', 'school_holidays'],\n      dtype='object')"},"metadata":{}}],"execution_count":66},{"cell_type":"code","source":"def create_date_features(df):\n    df['day_of_week'] = df['date'].dt.dayofweek\n    df['week_of_year'] = df['date'].dt.isocalendar().week.astype(int)\n    df['month'] = df['date'].dt.month\n    df['day'] = df['date'].dt.day\n    df['is_weekend'] = (df['day_of_week'] >= 5).astype(int)\n    return df\n\ntrain = create_date_features(train)\ntest = create_date_features(test)\n\n# Example: Create a feature that represents the maximum discount across discount columns\ndiscount_cols = [col for col in train.columns if 'type_' in col and 'discount' in col]\ntrain['max_discount'] = train[discount_cols].max(axis=1)\ntest['max_discount'] = test[discount_cols].max(axis=1)","metadata":{"_uuid":"4b5978dc-6524-44ff-a659-f9cd2296954a","_cell_guid":"c3c77edd-5758-4203-9c7c-23336e1c0335","trusted":true,"collapsed":false,"execution":{"iopub.status.busy":"2025-02-05T15:59:03.336625Z","iopub.execute_input":"2025-02-05T15:59:03.337123Z","iopub.status.idle":"2025-02-05T15:59:03.390387Z","shell.execute_reply.started":"2025-02-05T15:59:03.337072Z","shell.execute_reply":"2025-02-05T15:59:03.388765Z"},"jupyter":{"outputs_hidden":false}},"outputs":[],"execution_count":67},{"cell_type":"code","source":"def create_lag_features(df, lags, window):\n    # Sort by unique_id and date\n    df = df.sort_values(['unique_id', 'date'])\n    \n    for lag in lags:\n        df[f'lag_{lag}'] = df.groupby('unique_id')['sales'].shift(lag)\n    \n    # Rolling mean over a given window\n    df[f'rolling_mean_{window}'] = df.groupby('unique_id')['sales'].shift(1).rolling(window=window).mean()\n    \n    return df\n\n# Create lag features in training data only; note that for test you'll need a careful approach to use history.\n# Here we create lag features only on training data for simplicity.\ntrain = create_lag_features(train, lags=[7, 14], window=7)\n\n# Remove rows with NaN values that result from lag features (e.g., first 14 days per series)\ntrain = train.dropna(subset=['lag_7', 'lag_14', 'rolling_mean_7'])","metadata":{"_uuid":"56cd7949-63b2-4840-873f-ecd14f466cc0","_cell_guid":"60712bcc-81b8-4929-8295-9040f02595af","trusted":true,"collapsed":false,"execution":{"iopub.status.busy":"2025-02-05T15:59:03.393034Z","iopub.execute_input":"2025-02-05T15:59:03.393594Z","iopub.status.idle":"2025-02-05T15:59:03.503051Z","shell.execute_reply.started":"2025-02-05T15:59:03.393552Z","shell.execute_reply":"2025-02-05T15:59:03.501333Z"},"jupyter":{"outputs_hidden":false}},"outputs":[],"execution_count":68},{"cell_type":"code","source":"features = [\n    'total_orders', 'sell_price_main', 'availability', 'max_discount', \n    'day_of_week', 'week_of_year', 'month', 'day', 'is_weekend',\n    'lag_7', 'lag_14', 'rolling_mean_7'\n]\n\n# You might also want to add categorical features like warehouse, product category, etc.\n# Here, we use a simple encoding for the 'warehouse' column\ntrain['warehouse_enc'] = train['warehouse'].astype('category').cat.codes\ntest['warehouse_enc'] = test['warehouse'].astype('category').cat.codes\nfeatures.append('warehouse_enc')\n\ntarget = 'sales'\n\n# Use a time-based split: for example, use the last 20% of dates for validation.\n# Here we sort by date and split.\ntrain = train.sort_values('date')\nsplit_date = train['date'].quantile(0.8)\ntrain_data = train[train['date'] <= split_date]\nval_data = train[train['date'] > split_date]\n\nX_train = train_data[features]\ny_train = train_data[target]\nX_val = val_data[features]\ny_val = val_data[target]","metadata":{"_uuid":"5a58b549-4ddf-4083-aeec-e4ce68c39b82","_cell_guid":"091e6be9-710c-4cdc-8b0f-cdc17b98b2e0","trusted":true,"collapsed":false,"execution":{"iopub.status.busy":"2025-02-05T15:59:03.504909Z","iopub.execute_input":"2025-02-05T15:59:03.505400Z","iopub.status.idle":"2025-02-05T15:59:03.544662Z","shell.execute_reply.started":"2025-02-05T15:59:03.505359Z","shell.execute_reply":"2025-02-05T15:59:03.542926Z"},"jupyter":{"outputs_hidden":false}},"outputs":[],"execution_count":69},{"cell_type":"code","source":"params = {\n    'objective': 'regression',\n    'metric': 'mae',\n    'learning_rate': 0.05,\n    'num_leaves': 31,\n    'verbose': -1,\n    'seed': 42\n}","metadata":{"trusted":true,"execution":{"iopub.status.busy":"2025-02-05T15:59:03.551120Z","iopub.execute_input":"2025-02-05T15:59:03.551734Z","iopub.status.idle":"2025-02-05T15:59:03.558822Z","shell.execute_reply.started":"2025-02-05T15:59:03.551690Z","shell.execute_reply":"2025-02-05T15:59:03.556905Z"}},"outputs":[],"execution_count":70},{"cell_type":"code","source":"from sklearn.metrics import mean_absolute_error\n\n# 1. Train an initial model\ntrain_data = lgb.Dataset(X_train, label=y_train)\ninitial_model = lgb.train(params, train_data, num_boost_round=500)\n\n# 2. Compute feature importance\nimportance_df = pd.DataFrame({\n    'feature': X_train.columns,\n    'importance': initial_model.feature_importance(importance_type='gain')\n}).sort_values(by='importance', ascending=False)","metadata":{"trusted":true,"execution":{"iopub.status.busy":"2025-02-05T15:59:03.560366Z","iopub.execute_input":"2025-02-05T15:59:03.560728Z","iopub.status.idle":"2025-02-05T15:59:07.743249Z","shell.execute_reply.started":"2025-02-05T15:59:03.560695Z","shell.execute_reply":"2025-02-05T15:59:07.741324Z"}},"outputs":[],"execution_count":71},{"cell_type":"code","source":"importance_df","metadata":{"trusted":true,"execution":{"iopub.status.busy":"2025-02-05T15:59:07.744924Z","iopub.execute_input":"2025-02-05T15:59:07.745457Z","iopub.status.idle":"2025-02-05T15:59:07.761306Z","shell.execute_reply.started":"2025-02-05T15:59:07.745397Z","shell.execute_reply":"2025-02-05T15:59:07.758946Z"}},"outputs":[{"execution_count":72,"output_type":"execute_result","data":{"text/plain":"            feature    importance\n11   rolling_mean_7  2.721211e+09\n3      max_discount  5.602941e+08\n0      total_orders  1.256741e+08\n5      week_of_year  9.540776e+07\n9             lag_7  6.601292e+07\n10           lag_14  4.852625e+07\n1   sell_price_main  4.493074e+07\n7               day  3.729315e+07\n4       day_of_week  2.047512e+07\n2      availability  1.466351e+07\n6             month  6.758590e+06\n12    warehouse_enc  4.716782e+06\n8        is_weekend  0.000000e+00","text/html":"<div>\n<style scoped>\n    .dataframe tbody tr th:only-of-type {\n        vertical-align: middle;\n    }\n\n    .dataframe tbody tr th {\n        vertical-align: top;\n    }\n\n    .dataframe thead th {\n        text-align: right;\n    }\n</style>\n<table border=\"1\" class=\"dataframe\">\n  <thead>\n    <tr style=\"text-align: right;\">\n      <th></th>\n      <th>feature</th>\n      <th>importance</th>\n    </tr>\n  </thead>\n  <tbody>\n    <tr>\n      <th>11</th>\n      <td>rolling_mean_7</td>\n      <td>2.721211e+09</td>\n    </tr>\n    <tr>\n      <th>3</th>\n      <td>max_discount</td>\n      <td>5.602941e+08</td>\n    </tr>\n    <tr>\n      <th>0</th>\n      <td>total_orders</td>\n      <td>1.256741e+08</td>\n    </tr>\n    <tr>\n      <th>5</th>\n      <td>week_of_year</td>\n      <td>9.540776e+07</td>\n    </tr>\n    <tr>\n      <th>9</th>\n      <td>lag_7</td>\n      <td>6.601292e+07</td>\n    </tr>\n    <tr>\n      <th>10</th>\n      <td>lag_14</td>\n      <td>4.852625e+07</td>\n    </tr>\n    <tr>\n      <th>1</th>\n      <td>sell_price_main</td>\n      <td>4.493074e+07</td>\n    </tr>\n    <tr>\n      <th>7</th>\n      <td>day</td>\n      <td>3.729315e+07</td>\n    </tr>\n    <tr>\n      <th>4</th>\n      <td>day_of_week</td>\n      <td>2.047512e+07</td>\n    </tr>\n    <tr>\n      <th>2</th>\n      <td>availability</td>\n      <td>1.466351e+07</td>\n    </tr>\n    <tr>\n      <th>6</th>\n      <td>month</td>\n      <td>6.758590e+06</td>\n    </tr>\n    <tr>\n      <th>12</th>\n      <td>warehouse_enc</td>\n      <td>4.716782e+06</td>\n    </tr>\n    <tr>\n      <th>8</th>\n      <td>is_weekend</td>\n      <td>0.000000e+00</td>\n    </tr>\n  </tbody>\n</table>\n</div>"},"metadata":{}}],"execution_count":72},{"cell_type":"code","source":"# 3. Define a threshold (e.g., keep features with importance greater than the median importance)\nthreshold = 0\nselected_features = importance_df[importance_df['importance'] > threshold]['feature'].tolist()\nprint(\"Selected features:\", selected_features)","metadata":{"trusted":true,"execution":{"iopub.status.busy":"2025-02-05T15:59:07.763807Z","iopub.execute_input":"2025-02-05T15:59:07.764423Z","iopub.status.idle":"2025-02-05T15:59:07.774915Z","shell.execute_reply.started":"2025-02-05T15:59:07.764365Z","shell.execute_reply":"2025-02-05T15:59:07.773533Z"}},"outputs":[{"name":"stdout","text":"Selected features: ['rolling_mean_7', 'max_discount', 'total_orders', 'week_of_year', 'lag_7', 'lag_14', 'sell_price_main', 'day', 'day_of_week', 'availability', 'month', 'warehouse_enc']\n","output_type":"stream"}],"execution_count":73},{"cell_type":"code","source":"# 4. Subset the training and validation sets\nX_train_sel = X_train[selected_features]\nX_val_sel = X_val[selected_features]\n\n# 5. Retrain the model using the selected features\ntrain_data_sel = lgb.Dataset(X_train_sel, label=y_train)\nval_data_sel = lgb.Dataset(X_val_sel, label=y_val, reference=train_data_sel)\n\nfinal_model = lgb.train(\n    params,\n    train_data_sel,\n    valid_sets=[train_data_sel, val_data_sel],\n    num_boost_round=1000,\n    #early_stopping_rounds=50,\n    #verbose_eval=100\n)\n\n# 6. Evaluate the final model\nval_pred_sel = final_model.predict(X_val_sel, num_iteration=final_model.best_iteration)\nmae_sel = mean_absolute_error(y_val, val_pred_sel)\nprint(f'Validation MAE with selected features: {mae_sel:.4f}')","metadata":{"trusted":true,"execution":{"iopub.status.busy":"2025-02-05T15:59:07.776712Z","iopub.execute_input":"2025-02-05T15:59:07.777186Z","iopub.status.idle":"2025-02-05T15:59:19.212000Z","shell.execute_reply.started":"2025-02-05T15:59:07.777123Z","shell.execute_reply":"2025-02-05T15:59:19.210647Z"}},"outputs":[{"name":"stdout","text":"Validation MAE with selected features: 21.3841\n","output_type":"stream"}],"execution_count":74},{"cell_type":"code","source":"# Retrieve the feature names used during training\ntrained_features = final_model.feature_name()\nprint(\"Features used during training:\", trained_features)\n\n# Subset the test_final DataFrame to exactly these features and reset the index\nX_test = test_final[trained_features].reset_index(drop=True)\n\n# Debug: Check shape and duplicate columns\nprint(\"X_test shape:\", X_test.shape)\nduplicates = X_test.columns[X_test.columns.duplicated()].tolist()\nif duplicates:\n    print(\"Duplicate columns found:\", duplicates)\nelse:\n    print(\"No duplicate columns found.\")\n\n# Predict using the final model\nprint(\"Predicting test data...\")\ntest_final['sales_hat'] = final_model.predict(X_test, num_iteration=final_model.best_iteration)","metadata":{"trusted":true,"execution":{"iopub.status.busy":"2025-02-05T15:59:19.213619Z","iopub.execute_input":"2025-02-05T15:59:19.214080Z","iopub.status.idle":"2025-02-05T15:59:20.661736Z","shell.execute_reply.started":"2025-02-05T15:59:19.214030Z","shell.execute_reply":"2025-02-05T15:59:20.660604Z"}},"outputs":[{"name":"stdout","text":"Features used during training: ['rolling_mean_7', 'max_discount', 'total_orders', 'week_of_year', 'lag_7', 'lag_14', 'sell_price_main', 'day', 'day_of_week', 'availability', 'month', 'warehouse_enc']\nX_test shape: (47021, 12)\nNo duplicate columns found.\nPredicting test data...\n","output_type":"stream"}],"execution_count":75},{"cell_type":"code","source":"# Create a DataFrame for submission\ntest_final['id'] = test['unique_id'].astype(str) + '_' + test['date'].dt.strftime('%Y-%m-%d')\nsubmission = test_final[['id', 'sales_hat']]\n\n# Save to CSV\nsubmission.to_csv('submission.csv', index=False)\n\nprint(\"Submission file saved as submission.csv\")","metadata":{"trusted":true,"execution":{"iopub.status.busy":"2025-02-05T16:03:14.180764Z","iopub.execute_input":"2025-02-05T16:03:14.181341Z","iopub.status.idle":"2025-02-05T16:03:14.350774Z","shell.execute_reply.started":"2025-02-05T16:03:14.181290Z","shell.execute_reply":"2025-02-05T16:03:14.349181Z"}},"outputs":[{"name":"stdout","text":"Submission file saved as submission.csv\n","output_type":"stream"}],"execution_count":77},{"cell_type":"code","source":"# -------------------------------\n# Cleanup\n# -------------------------------\ngc.collect()","metadata":{"trusted":true,"execution":{"iopub.status.busy":"2025-02-05T16:03:16.816915Z","iopub.execute_input":"2025-02-05T16:03:16.817383Z","iopub.status.idle":"2025-02-05T16:03:17.068869Z","shell.execute_reply.started":"2025-02-05T16:03:16.817344Z","shell.execute_reply":"2025-02-05T16:03:17.067222Z"}},"outputs":[{"execution_count":78,"output_type":"execute_result","data":{"text/plain":"985"},"metadata":{}}],"execution_count":78},{"cell_type":"code","source":"","metadata":{"trusted":true},"outputs":[],"execution_count":null}]}